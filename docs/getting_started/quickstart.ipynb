{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Sinabs\n",
    "If you're familiar with how SNNs work, you might find this quick overview about *Sinabs* useful.\n",
    "\n",
    "## Sinabs is based on PyTorch\n",
    "All of Sinabs' layers inherit from `torch.nn.Module`. Thus you will be able to access your parameters, wrap layers in a `nn.Sequential` module and all the other things that you would do with a normal PyTorch layer. \n",
    "\n",
    "## How to define your network\n",
    "We want to re-use as much PyTorch functionality as possible. We use Linear, Conv2d and AvgPool layers to define weight matrices, whereas *Sinabs* layers add state as well as the non-linear activation to each of those weight layers. This is a definition of a simple SNN which takes as an input a tensor of (Batch, Time, Channels):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sinabs.activation\n",
    "import sinabs.layers as sl\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 64),\n",
    "    sl.LIF(tau_mem=10., norm_input=True, activation_fn=sinabs.activation.ActivationFunction(surrogate_grad_fn=sinabs.activation.SingleExponential())),\n",
    "    #sl.LIF(tau_mem=10., norm_input=False),\n",
    "    nn.Linear(64, 4),\n",
    "    sl.LIF(tau_mem=10., norm_input=True, activation_fn=sinabs.activation.ActivationFunction(surrogate_grad_fn=sinabs.activation.SingleExponential())),\n",
    "    #sl.LIF(tau_mem=10., norm_input=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference with SNNs\n",
    "For simple inference using SNNs, you just use the model like any other torch model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Define an input (Batch, Time, Channels)\n",
    "input = (torch.rand(1, 100, 16) > 0.2).float()\n",
    "\n",
    "# Compute output with the model\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "\n",
    "print(output.sum())  # You would expect an output of shape (batch_size*time_steps, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the network state is retained after any forward pass/inference. If you require resetting of the states/gradient, you can do so using the corresponding methods `layer.reset_states()` or `layer.zero_grad()`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with BPTT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "# Some helper functions to reset our model during the training loops\n",
    "def reset_model_states(seq_model: nn.Sequential, randomize: bool=False):\n",
    "    \"\"\"\n",
    "    Method to reset the internal states of a model\n",
    "    \"\"\"\n",
    "    for lyr in seq_model:\n",
    "        if isinstance(lyr, sl.LIF):\n",
    "            lyr.reset_states(randomize=randomize)\n",
    "    return\n",
    "\n",
    "\n",
    "def zero_grad_states(seq_model: nn.Sequential):\n",
    "    \"\"\"\n",
    "    Method to reset the gradients of the internal states of a model\n",
    "    \"\"\"\n",
    "    for lyr in seq_model:\n",
    "        if isinstance(lyr, sl.LIF):\n",
    "            lyr.zero_grad()\n",
    "    return\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Output spikes: 0.0\n",
      "-6.826314449310303\n",
      "Epoch 1: Output spikes: 0.0\n",
      "-16.426284790039062\n",
      "Epoch 2: Output spikes: 33.0\n",
      "11.343732833862305\n",
      "Epoch 3: Output spikes: 0.0\n",
      "-18.771665573120117\n",
      "Epoch 4: Output spikes: 0.0\n",
      "-24.664527893066406\n",
      "Epoch 5: Output spikes: 0.0\n",
      "-24.771141052246094\n",
      "Epoch 6: Output spikes: 12.0\n",
      "-20.896390914916992\n",
      "Epoch 7: Output spikes: 8.0\n",
      "-22.420576095581055\n",
      "Epoch 8: Output spikes: 12.0\n",
      "-21.05232810974121\n",
      "Epoch 9: Output spikes: 8.0\n",
      "-22.574174880981445\n",
      "Epoch 10: Output spikes: 11.0\n",
      "-21.21195411682129\n",
      "Epoch 11: Output spikes: 11.0\n",
      "-21.968189239501953\n",
      "Epoch 12: Output spikes: 8.0\n",
      "-22.677114486694336\n",
      "Epoch 13: Output spikes: 11.0\n",
      "-21.308813095092773\n",
      "Epoch 14: Output spikes: 11.0\n",
      "-22.068706512451172\n",
      "Epoch 15: Output spikes: 8.0\n",
      "-22.781986236572266\n",
      "Epoch 16: Output spikes: 12.0\n",
      "-21.404481887817383\n",
      "Epoch 17: Output spikes: 8.0\n",
      "-22.94760513305664\n",
      "Epoch 18: Output spikes: 11.0\n",
      "-21.57461929321289\n",
      "Epoch 19: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 20: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 21: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 22: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 23: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 24: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 25: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 26: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 27: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 28: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 29: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 30: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 31: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 32: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 33: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 34: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 35: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 36: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 37: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 38: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 39: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 40: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 41: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 42: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 43: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 44: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 45: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 46: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 47: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 48: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 49: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 50: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 51: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 52: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 53: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 54: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 55: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 56: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 57: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 58: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 59: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 60: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 61: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 62: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 63: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 64: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 65: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 66: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 67: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 68: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 69: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 70: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 71: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 72: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 73: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 74: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 75: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 76: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 77: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 78: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 79: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 80: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 81: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 82: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 83: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 84: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 85: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 86: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 87: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 88: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 89: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 90: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 91: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 92: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 93: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 94: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 95: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 96: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 97: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 98: Output spikes: 10.0\n",
      "-22.338441848754883\n",
      "Epoch 99: Output spikes: 10.0\n",
      "-22.338441848754883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Training routine\n",
    "optim = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "target_num_spikes = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset the gradients of the parameters\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # We will also need to reset the gradients of neuron states.\n",
    "    zero_grad_states(model)\n",
    "    # Alternatively you could also reset the states themselves.\n",
    "    reset_model_states(model, randomize=False)\n",
    "\n",
    "    out = model(input)\n",
    "    print(f\"Epoch {epoch}: Output spikes: {out.sum().item()}\")\n",
    "    loss = (out.sum() - target_num_spikes)**2\n",
    "    print(model[0].weight.sum().item())\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Early stopage\n",
    "    #if not loss:\n",
    "    #    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(10., grad_fn=<SumBackward0>), torch.Size([1, 100, 4]))"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sum(), out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with Convolutional networks\n",
    "\n",
    "When working with convolutional connectivity, a `nn.Conv2d` layer only takes as input a tensor of (Batch, Channels, Height, Width). If we feed a tensor that has an additional time dimension (Batch, Time, Channels, Height, Width) to such a layer, we will receive an error. In order for us to apply 2D convolutions across time, we have to make use of a small trick where we flatten batch and time dimension before feeding it to the Conv layer. If the input is flattened, the `Squeeze` versions of spiking `Sinabs` layers understand and take care of expanding the time dimension appropriately, without any major changes to your model definition."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "time_steps = 100\n",
    "\n",
    "conv_model = nn.Sequential(\n",
    "    nn.Conv2d(2, 16, kernel_size=3),\n",
    "    sl.LIFSqueeze(tau_mem=20., batch_size=batch_size),\n",
    "    nn.Conv2d(16, 32, kernel_size=3),\n",
    "    sl.LIFSqueeze(tau_mem=20., batch_size=batch_size),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, 4),\n",
    ")\n",
    "\n",
    "# (Batch*Time, Channels, Height, Width)\n",
    "data = torch.rand(batch_size, time_steps, 2, 8, 8)\n",
    "\n",
    "# Data reshaped to fit the flattened model definition\n",
    "input = data.resize(batch_size*time_steps, 2, 8, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rest of the forward pass or training loops remain the same as described in the above sections."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = conv_model(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This output can then be reshaped to split the dimensions between batch and time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "output_spike_raster = output.reshape(batch_size, time_steps, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5931241afb711235dda64ee4fe99a453ecee36036d1d9ee62f788faeb386adff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}