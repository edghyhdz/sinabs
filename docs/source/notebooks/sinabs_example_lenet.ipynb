{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate the various steps involved in implementing a LeNet 5 model with sinabs. This tutorial will have the following steps.\n",
    "\n",
    "1. Build/define CNN and SNN models.\n",
    "2. Load weights.\n",
    "3. Load data.\n",
    "4. Tuning/scaling the parameters for the SNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by importing all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sinabs.layers as sl\n",
    "import sinabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define equivalent SNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an equivalent SNN model by using the SNN equivalent layers which are conveniently named as `Spiking<Operation>Layer` in `sinabs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MySNN, self).__init__()\n",
    "        # Spiking Input Layer\n",
    "        self.input_1 = sl.InputLayer(input_shape=(1, 28, 28), layer_name=\"input_1\")\n",
    "\n",
    "        # Spiking Conv layer\n",
    "        self.conv1 = sl.SpikingConv2dLayer(\n",
    "            channels_in=1,\n",
    "            image_shape=(28, 28),\n",
    "            channels_out=20,\n",
    "            kernel_shape=(5, 5),\n",
    "            layer_name=\"conv1\",\n",
    "            strides=(1,1),\n",
    "        )\n",
    "\n",
    "        # Spiking SumPooling layer\n",
    "        self.pool1 = sl.SpikingMaxPooling2dLayer(\n",
    "            image_shape=(24, 24), pool_size=(2, 2), layer_name=\"pool1\"\n",
    "        )\n",
    "\n",
    "        # Spiking Conv layer\n",
    "        self.conv2 = sl.SpikingConv2dLayer(\n",
    "            channels_in=20,\n",
    "            image_shape=(12, 12),\n",
    "            channels_out=50,\n",
    "            kernel_shape=(5, 5),\n",
    "            layer_name=\"conv2\",\n",
    "        )\n",
    "\n",
    "        # Spiking SumPooling layer\n",
    "        self.pool2 = sl.SpikingMaxPooling2dLayer(\n",
    "            image_shape=(8, 8), pool_size=(2, 2), layer_name=\"pool2\"\n",
    "        )\n",
    "\n",
    "        # Generating an Equivalent Spiking Dense Layer\n",
    "        # This layer emulates flatten+dense layer        \n",
    "        self.fc1= sl.SpikingConv2dLayer(channels_in=50,\n",
    "                                         image_shape=(4,4),\n",
    "                                         channels_out=500,\n",
    "                                         kernel_shape=(4,4),\n",
    "                                         strides=(4,4),\n",
    "                                         layer_name='fc1'\n",
    "                                        )\n",
    "        self.fc2 = sl.SpikingConv2dLayer(channels_in=500,\n",
    "                                 image_shape=(1,1),\n",
    "                                 channels_out=10,\n",
    "                                 kernel_shape=(1,1),\n",
    "                                 strides=(1,1),\n",
    "                                 layer_name='fc2'\n",
    "                                )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define your graph\n",
    "        x = self.input_1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        #x = self.flatten1(x)\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE` : When assigning `layer_name` in your model, use the same name as the variable name assigned in the module. The naming is used to identify the layers when comparing CNN with SNN in `sinabs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the spiking model and  encapsulate into Network object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = sinabs.Network()\n",
    "snn.spiking_model = MySNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadique/Work/aictx/sinabs/sinabs/network.py:381: UserWarning: Graph couldn't be inferred by sinabs. Perhaps you have defined a custom model.\n",
      "  \"Graph couldn't be inferred by sinabs. Perhaps you have defined a custom model.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias_Params</th>\n",
       "      <th>Fanout_Prev</th>\n",
       "      <th>Input_Shape</th>\n",
       "      <th>Kernel_Params</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Output_Shape</th>\n",
       "      <th>Type</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Padding</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Pooling</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 28, 28)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1, 28, 28)</td>\n",
       "      <td>InputLayer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>(1, 28, 28)</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>(20, 24, 24)</td>\n",
       "      <td>SpikingConv2dLayer</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pool1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(None, 24, 24)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(None, 12, 12)</td>\n",
       "      <td>SpikingMaxPooling2dLayer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>(20, 12, 12)</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>(50, 8, 8)</td>\n",
       "      <td>SpikingConv2dLayer</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pool2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(None, 8, 8)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(None, 4, 4)</td>\n",
       "      <td>SpikingMaxPooling2dLayer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc1</th>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>(50, 4, 4)</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>(500, 1, 1)</td>\n",
       "      <td>SpikingConv2dLayer</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>(500, 1, 1)</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>(10, 1, 1)</td>\n",
       "      <td>SpikingConv2dLayer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Bias_Params  Fanout_Prev     Input_Shape  Kernel_Params  Neurons  \\\n",
       "Layer                                                                       \n",
       "input_1          0.0          1.0     (1, 28, 28)            0.0      0.0   \n",
       "conv1           20.0        500.0     (1, 28, 28)          500.0  11520.0   \n",
       "pool1            0.0          1.0  (None, 24, 24)            0.0      0.0   \n",
       "conv2           50.0       1250.0    (20, 12, 12)        25000.0   3200.0   \n",
       "pool2            0.0          1.0    (None, 8, 8)            0.0      0.0   \n",
       "fc1            500.0        500.0      (50, 4, 4)       400000.0    500.0   \n",
       "fc2             10.0         10.0     (500, 1, 1)         5000.0     10.0   \n",
       "\n",
       "           Output_Shape                      Type  Kernel       Padding  \\\n",
       "Layer                                                                     \n",
       "input_1     (1, 28, 28)                InputLayer     NaN           NaN   \n",
       "conv1      (20, 24, 24)        SpikingConv2dLayer  (5, 5)  (0, 0, 0, 0)   \n",
       "pool1    (None, 12, 12)  SpikingMaxPooling2dLayer  (2, 2)  (0, 0, 0, 0)   \n",
       "conv2        (50, 8, 8)        SpikingConv2dLayer  (5, 5)  (0, 0, 0, 0)   \n",
       "pool2      (None, 4, 4)  SpikingMaxPooling2dLayer  (2, 2)  (0, 0, 0, 0)   \n",
       "fc1         (500, 1, 1)        SpikingConv2dLayer  (4, 4)  (0, 0, 0, 0)   \n",
       "fc2          (10, 1, 1)        SpikingConv2dLayer  (1, 1)  (0, 0, 0, 0)   \n",
       "\n",
       "         Stride Pooling  \n",
       "Layer                    \n",
       "input_1     NaN     NaN  \n",
       "conv1    (1, 1)     NaN  \n",
       "pool1    (2, 2)  (2, 2)  \n",
       "conv2    (1, 1)     NaN  \n",
       "pool2    (2, 2)  (2, 2)  \n",
       "fc1      (4, 4)     NaN  \n",
       "fc2      (1, 1)     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize an equivalent CNN model and encapsulate it to the network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_net = CNN()\n",
    "cnn_net.load_state_dict(torch.load(\"./mnist_cnn.pt\"))\n",
    "\n",
    "# Add a reference to the CNN model to the Network object\n",
    "snn.analog_model = cnn_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a reference to the equivalent CNN model in the Network object allows several methods in `Network` object. One such method is the `set_weights` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine some of the parameters of the SNN and CNN models we initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
      "=======================\n",
      "Parameter containing:\n",
      "tensor([-0.0847, -0.0368, -0.0393,  0.1194,  0.0671, -0.0623, -0.0644,  0.0233,\n",
      "         0.1963,  0.0918, -0.1171,  0.1657, -0.0825,  0.0382, -0.0547, -0.0136,\n",
      "         0.1462, -0.1023,  0.0438,  0.0394], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weights = cnn_net.state_dict()\n",
    "print(weights.keys())\n",
    "print(\"=======================\")\n",
    "print(list(snn.spiking_model.parameters())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transfer the weights of the CNN model to the SNN model. To do this, we first extract the weights of the CNN model into a list of numpy arrays holding all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from CNN model\n",
    "weights = [p.detach().numpy() for p in cnn_net.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights to both CNN and SNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the biases of the first layer in the analog and CNN model. \n",
    "NOTE: We set `auto_rescale` here to `False`. We will see what parameter means later in this tutorial. \n",
    "Usually we want this parameter to be set to `True`. It is set to `False` only for illustrative purposes of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.2348e-01, -1.4768e-01, -1.7782e-01, -5.0483e-02, -3.9126e-02,\n",
      "        -9.7103e-02, -1.0429e-04, -9.4956e-02,  2.7214e-02, -1.4487e-03,\n",
      "        -1.0568e-01,  2.2480e-02, -9.4489e-02,  2.1322e-01,  7.6859e-02,\n",
      "        -1.7281e-01,  2.0359e-03, -6.6384e-02, -1.5167e-01, -9.1698e-03],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(list(snn.spiking_model.parameters())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.2348e-01, -1.4768e-01, -1.7782e-01, -5.0483e-02, -3.9126e-02,\n",
      "        -9.7103e-02, -1.0429e-04, -9.4956e-02,  2.7214e-02, -1.4487e-03,\n",
      "        -1.0568e-01,  2.2480e-02, -9.4489e-02,  2.1322e-01,  7.6859e-02,\n",
      "        -1.7281e-01,  2.0359e-03, -6.6384e-02, -1.5167e-01, -9.1698e-03],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(list(cnn_net.parameters())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the parameters have been successfully transferred from the analog model to the spiking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate input for the spiking model and analog model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by define a dataset for MNIST images. \n",
    "Our dataset generator will generate spikes or images depending on the initialization flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "class MNIST_Dataset(datasets.MNIST):\n",
    "    \n",
    "    def __init__(self, root, train = True, spiking=False, tWindow=100):\n",
    "        datasets.MNIST.__init__(self, root, train=train, download=True)\n",
    "        self.spiking=spiking\n",
    "        self.tWindow = tWindow\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        if self.spiking:\n",
    "            img = (np.random.rand(self.tWindow, 1, *img.size()) < img.numpy()/255.0).astype(float)\n",
    "            img = torch.from_numpy(img).float()\n",
    "        else:\n",
    "            # doing this so that it is consistent with all other datasets\n",
    "            # to return a PIL Image\n",
    "            img = torch.from_numpy(img.numpy()).float()\n",
    "            img.unsqueeze_(0)\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the accuracy of our model, we first define a convenience method to test and present its performance accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Convenience method to test the model\n",
    "def test(model, data_loader, num_batches=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate over data\n",
    "        for data, target in data_loader:\n",
    "            if data_loader.dataset.spiking:\n",
    "                if len(data.size()) > 4:\n",
    "                    warnings.warn(\"Warning: Batch size needs to be 1, only first sample used.\")\n",
    "                    data = data[0]\n",
    "                    target = target[0]\n",
    "            output = model(data)\n",
    "            if data_loader.dataset.spiking:\n",
    "                output = output.sum(0).squeeze().unsqueeze(0)\n",
    "                target = target.unsqueeze(0)\n",
    "            # Add loss to the total loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Compute the total correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            batch_count += 1\n",
    "            if (batch_count*data_loader.batch_size)%500 == 0:\n",
    "                print(f\"Accuracy : {correct/(batch_count*data_loader.batch_size):.4f} for {batch_count*data_loader.batch_size} samples\")\n",
    "            if num_batches:\n",
    "                if num_batches <= batch_count: break;\n",
    "       \n",
    "    # Total samples:\n",
    "    num_data = (batch_count*data_loader.batch_size)\n",
    "    # Compute final loss\n",
    "    test_loss /= num_data\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, num_data,\n",
    "        100. * correct / num_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first test the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test dataset loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_Dataset('./data', train=False, spiking=False),\n",
    "    batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9860 for 500 samples\n",
      "Accuracy : 0.9840 for 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9793 for 1500 samples\n",
      "Accuracy : 0.9795 for 2000 samples\n",
      "Accuracy : 0.9776 for 2500 samples\n",
      "Accuracy : 0.9780 for 3000 samples\n",
      "Accuracy : 0.9789 for 3500 samples\n",
      "Accuracy : 0.9785 for 4000 samples\n",
      "Accuracy : 0.9778 for 4500 samples\n",
      "Accuracy : 0.9774 for 5000 samples\n",
      "Accuracy : 0.9791 for 5500 samples\n",
      "Accuracy : 0.9798 for 6000 samples\n",
      "Accuracy : 0.9812 for 6500 samples\n",
      "Accuracy : 0.9820 for 7000 samples\n",
      "Accuracy : 0.9832 for 7500 samples\n",
      "Accuracy : 0.9842 for 8000 samples\n",
      "Accuracy : 0.9848 for 8500 samples\n",
      "Accuracy : 0.9856 for 9000 samples\n",
      "Accuracy : 0.9859 for 9500 samples\n",
      "Accuracy : 0.9857 for 10000 samples\n",
      "\n",
      "Test set: Average loss: -0.9857, Accuracy: 9857/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(cnn_net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us repeat this on the SCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window per sample\n",
    "tWindow = 50 # ms (or) time steps\n",
    "\n",
    "# Define test dataset loader\n",
    "test_spike_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_Dataset('./data', train=False, spiking=True, tWindow=tWindow),\n",
    "    batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "The following simulation run is going to be very slow. After a few iterations, the accuracy should stablize. \n",
    "Once you get a sense of the performance of the model. Just stop the run (stop button on notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Warning: Batch size needs to be 1, only first sample used.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: -1722.3600, Accuracy: 199/200 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(snn, test_spike_loader, num_batches=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a reasonable accuracy for the spiking model with equivalent performance to that of the CNN.\n",
    "\n",
    "Go to the bottom of this example to read a few pointers on how you can improve the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a synopsis of the total synaptic operations of this network by calling the `get_synops` method of the `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Events_routed</th>\n",
       "      <th>Fanout_Prev</th>\n",
       "      <th>In</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Out</th>\n",
       "      <th>SynOps</th>\n",
       "      <th>SynOps/s</th>\n",
       "      <th>Time_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>input_1</td>\n",
       "      <td>7461.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3730500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7461.0</td>\n",
       "      <td>conv1</td>\n",
       "      <td>99752.0</td>\n",
       "      <td>3730500.0</td>\n",
       "      <td>7.461000e+07</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99752.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99752.0</td>\n",
       "      <td>pool1</td>\n",
       "      <td>41811.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52263750.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>41811.0</td>\n",
       "      <td>conv2</td>\n",
       "      <td>54483.0</td>\n",
       "      <td>52263750.0</td>\n",
       "      <td>1.045275e+09</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54483.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54483.0</td>\n",
       "      <td>pool2</td>\n",
       "      <td>30223.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15111500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>30223.0</td>\n",
       "      <td>fc1</td>\n",
       "      <td>52566.0</td>\n",
       "      <td>15111500.0</td>\n",
       "      <td>3.022300e+08</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>525660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>52566.0</td>\n",
       "      <td>fc2</td>\n",
       "      <td>3887.0</td>\n",
       "      <td>525660.0</td>\n",
       "      <td>1.051320e+07</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Events_routed  Fanout_Prev       In    Layer      Out      SynOps  \\\n",
       "0           -1.0          1.0     -1.0  input_1   7461.0         0.0   \n",
       "1      3730500.0        500.0   7461.0    conv1  99752.0   3730500.0   \n",
       "2        99752.0          1.0  99752.0    pool1  41811.0         0.0   \n",
       "3     52263750.0       1250.0  41811.0    conv2  54483.0  52263750.0   \n",
       "4        54483.0          1.0  54483.0    pool2  30223.0         0.0   \n",
       "5     15111500.0        500.0  30223.0      fc1  52566.0  15111500.0   \n",
       "6       525660.0         10.0  52566.0      fc2   3887.0    525660.0   \n",
       "\n",
       "       SynOps/s  Time_window  \n",
       "0  0.000000e+00         50.0  \n",
       "1  7.461000e+07         50.0  \n",
       "2  0.000000e+00         50.0  \n",
       "3  1.045275e+09         50.0  \n",
       "4  0.000000e+00         50.0  \n",
       "5  3.022300e+08         50.0  \n",
       "6  1.051320e+07         50.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn.get_synops(num_evs_in=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the accuracy of the SNN on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Warning: Batch size needs to be 1, only first sample used.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9880 for 500 samples\n",
      "Accuracy : 0.9840 for 1000 samples\n",
      "Accuracy : 0.9800 for 1500 samples\n",
      "Accuracy : 0.9795 for 2000 samples\n",
      "Accuracy : 0.9788 for 2500 samples\n",
      "Accuracy : 0.9793 for 3000 samples\n",
      "Accuracy : 0.9797 for 3500 samples\n",
      "Accuracy : 0.9792 for 4000 samples\n",
      "Accuracy : 0.9787 for 4500 samples\n",
      "Accuracy : 0.9776 for 5000 samples\n",
      "Accuracy : 0.9793 for 5500 samples\n",
      "Accuracy : 0.9800 for 6000 samples\n",
      "Accuracy : 0.9814 for 6500 samples\n",
      "Accuracy : 0.9817 for 7000 samples\n",
      "Accuracy : 0.9828 for 7500 samples\n",
      "Accuracy : 0.9839 for 8000 samples\n",
      "Accuracy : 0.9845 for 8500 samples\n",
      "Accuracy : 0.9852 for 9000 samples\n",
      "Accuracy : 0.9855 for 9500 samples\n",
      "Accuracy : 0.9854 for 10000 samples\n",
      "\n",
      "Test set: Average loss: -1733.4155, Accuracy: 9854/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(snn, test_spike_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging tools\n",
    "\n",
    "The library provides some handy tools to verify the activity of the SNN network and compare it to CNN activations. The key method that allows you to do this is the `Netowrk.compare_activations()` method. This method returns the activations of any layer of interest form both CNN and SNN models.\n",
    "\n",
    "The `Network.plot_comparison()` plots the activations of both versions of the network in an understandable figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1', 'conv2', 'fc1', 'fc2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/sadique/Work/aictx/sinabs/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate data to use\n",
    "for data, label in test_spike_loader:\n",
    "    break;\n",
    " \n",
    "cnn_act, spk_act = snn.plot_comparison(data[0], compute_rate=True, name_list=['conv1', 'conv2', 'fc1', 'fc2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above figure, the activation of CNN and SNN fall on the $x=y$ diagonal line. This is the ideal case where the activity of the SNN faithfully replicates the CNN network. (The negative activations are not identical because of ReLu activation of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors influencing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see disappointing results or just about chance level accuracy, this might be due to several reasons.\n",
    "\n",
    "When you flatten the activity of your model, you need to be extremely cautious about order in which the layer activations are ordered. You will need to ensure that the ordering of the trained model is consistent with the SNN.\n",
    "\n",
    "It could be that the activity levels are very low and you need to `rescale` your weights when you set them for the spiking model. This happends when using pooling layers since in the spiking models, we use `SumPooling` in place of `AveragePooling`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
